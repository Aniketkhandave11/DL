{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "So-ilnKD0b31",
        "outputId": "0f57e7af-22f3-45ca-f0de-9cad69432713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<ipython-input-1-7297b7091bf9>:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y_data = y_data.replace({'positive': 1, 'negative': 0})\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.6145 - loss: 0.6166\n",
            "Epoch 1: val_accuracy improved from -inf to 0.81275, saving model to models/LSTM.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 200ms/step - accuracy: 0.6149 - loss: 0.6162 - val_accuracy: 0.8127 - val_loss: 0.4227\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8751 - loss: 0.3138\n",
            "Epoch 2: val_accuracy improved from 0.81275 to 0.86650, saving model to models/LSTM.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 208ms/step - accuracy: 0.8751 - loss: 0.3138 - val_accuracy: 0.8665 - val_loss: 0.3418\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8931 - loss: 0.2758\n",
            "Epoch 3: val_accuracy improved from 0.86650 to 0.87250, saving model to models/LSTM.keras\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 209ms/step - accuracy: 0.8931 - loss: 0.2758 - val_accuracy: 0.8725 - val_loss: 0.3288\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9046 - loss: 0.2484\n",
            "Epoch 4: val_accuracy did not improve from 0.87250\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 212ms/step - accuracy: 0.9046 - loss: 0.2485 - val_accuracy: 0.8708 - val_loss: 0.3394\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9139 - loss: 0.2253\n",
            "Epoch 5: val_accuracy did not improve from 0.87250\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 211ms/step - accuracy: 0.9139 - loss: 0.2253 - val_accuracy: 0.8665 - val_loss: 0.3514\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step\n",
            "Correct Predictions: 8588\n",
            "Wrong Predictions: 1412\n",
            "Accuracy: 85.88%\n",
            "Enter a movie review: movie is worst ive have been seeen ever\n",
            "Cleaned Review: movie worst ive seeen ever\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "Prediction Probability: 0.023646224\n",
            "Sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematical operations\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split  # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # for padding\n",
        "from tensorflow.keras.models import Sequential, load_model  # for building and loading models\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense  # for defining layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint  # to save the model during training\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "# Load the IMDB dataset\n",
        "def load_dataset():\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv('/content/IMDB Dataset.csv')  # Adjust path if not on Colab\n",
        "\n",
        "    # Ensure the dataset has the correct columns\n",
        "    if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
        "        raise ValueError(\"The dataset must contain 'review' and 'sentiment' columns.\")\n",
        "\n",
        "    # Extract reviews and sentiments\n",
        "    x_data = df['review']  # This will be a Pandas Series\n",
        "    y_data = df['sentiment']  # This will be a Pandas Series\n",
        "\n",
        "    # Preprocess reviews\n",
        "    x_data = x_data.str.replace(r'<.*?>', '', regex=True)  # Remove HTML tags\n",
        "    x_data = x_data.str.replace(r'[^A-Za-z\\s]', '', regex=True)  # Remove non-alphabet characters\n",
        "    x_data = x_data.str.lower()  # Convert to lowercase\n",
        "    x_data = x_data.apply(lambda review: ' '.join([w for w in review.split() if w not in english_stops]))  # Remove stopwords\n",
        "\n",
        "    # Encode sentiments: 'positive' -> 1, 'negative' -> 0\n",
        "    y_data = y_data.replace({'positive': 1, 'negative': 0})\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "# Load and preprocess data\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to compute the maximum sequence length\n",
        "def get_max_length(data):\n",
        "    # Ensure each review is a string\n",
        "    if isinstance(data, pd.Series):\n",
        "        review_lengths = data.apply(lambda x: len(str(x).split()))  # Convert each review to string before split\n",
        "    else:\n",
        "        # Handle the case when data is a list, using list comprehension\n",
        "        review_lengths = [len(str(x).split()) for x in data]\n",
        "\n",
        "    return int(np.ceil(np.mean(review_lengths)))\n",
        "\n",
        "# Now call this function using x_train (which should be a Pandas Series)\n",
        "max_length = get_max_length(x_train)\n",
        "\n",
        "# Encode reviews using a tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)  # Use only top 5000 words\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences to ensure consistent input length\n",
        "max_length = get_max_length(x_train)\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Get vocabulary size for the embedding layer\n",
        "total_words = len(tokenizer.word_index) + 1  # Include padding token\n",
        "\n",
        "# Define the model\n",
        "EMBED_DIM = 32\n",
        "LSTM_OUT = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=EMBED_DIM, input_length=max_length))\n",
        "model.add(LSTM(LSTM_OUT))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Ensure the 'models' directory exists\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Define a checkpoint callback to save the best model\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='models/LSTM.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.1,  # Use 10% of training data for validation\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(x_test, batch_size=128) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = np.sum(y_pred.flatten() == y_test.values)\n",
        "accuracy = correct_predictions / len(y_test) * 100\n",
        "\n",
        "print(f'Correct Predictions: {correct_predictions}')\n",
        "print(f'Wrong Predictions: {len(y_test) - correct_predictions}')\n",
        "print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Load the best saved model\n",
        "loaded_model = load_model('models/LSTM.keras')\n",
        "\n",
        "# Function for predicting sentiment of a new review\n",
        "def predict_sentiment(review):\n",
        "    # Pre-process input\n",
        "    review = re.sub(r'<.*?>', '', review)  # Remove HTML tags\n",
        "    review = re.sub(r'[^a-zA-Z\\s]', '', review)  # Remove non-alphabet characters\n",
        "    review = ' '.join([w for w in review.split() if w.lower() not in english_stops])  # Remove stopwords\n",
        "    review = review.lower()  # Convert to lowercase\n",
        "\n",
        "    print('Cleaned Review:', review)\n",
        "\n",
        "    # Tokenize and pad the review\n",
        "    tokenize_words = tokenizer.texts_to_sequences([review])\n",
        "    tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Predict sentiment\n",
        "    result = loaded_model.predict(tokenize_words)[0][0]\n",
        "    print('Prediction Probability:', result)\n",
        "\n",
        "    if result >= 0.5:\n",
        "        print('Sentiment: Positive')\n",
        "    else:\n",
        "        print('Sentiment: Negative')\n",
        "\n",
        "# Test the function\n",
        "review = input('Enter a movie review: ')\n",
        "predict_sentiment(review)\n"
      ]
    }
  ]
}